{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33d0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"dataartist/arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40409ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "trainset = [\n",
    "    dspy.Example(\n",
    "        training_examples=ex[\"train\"],\n",
    "        test_inputs=[x[\"input\"] for x in ex[\"test\"]],\n",
    "        test_outputs=[x[\"output\"] for x in ex[\"test\"]],\n",
    "    ).with_inputs(\"training_examples\", \"test_inputs\")\n",
    "    for ex in ds[\"training\"]\n",
    "]\n",
    "testset = [\n",
    "    dspy.Example(\n",
    "        training_examples=ex[\"train\"],\n",
    "        test_inputs=[x[\"input\"] for x in ex[\"test\"]],\n",
    "        test_outputs=[x[\"output\"] for x in ex[\"test\"]],\n",
    "    ).with_inputs(\"training_examples\", \"test_inputs\")\n",
    "    for ex in ds[\"evaluation\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78482f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ec6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_src = \"\"\"import dspy\n",
    "from typing import List\n",
    "import pydantic\n",
    "\n",
    "MATRIX = List[List[int]]\n",
    "\n",
    "class TrainingExample(pydantic.BaseModel):\n",
    "    input: MATRIX\n",
    "    output: MATRIX\n",
    "\n",
    "class SolveTaskSignature(dspy.Signature):\n",
    "    training_examples: List[TrainingExample] = dspy.InputField(description=\"Input and output examples demonstrating the task to be performed.\")\n",
    "    test_inputs: List[MATRIX] = dspy.InputField(description=\"Input matrices to be solved following the task described in the training examples.\")\n",
    "    test_outputs: List[MATRIX] = dspy.OutputField(description=\"Output matrices corresponding to the test inputs.\")\n",
    "\n",
    "program = dspy.ChainOfThought(SolveTaskSignature)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2140cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "003e479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_matrix(matrix, gold_matrix):\n",
    "    if not isinstance(matrix, list):\n",
    "        return False, f\"The matrix must be a List[List[int]]. The correct matrix is {gold_matrix}.\"\n",
    "    n = len(matrix)\n",
    "    if n == 0:\n",
    "        return False, f\"The matrix must have at least one row. The correct matrix is {gold_matrix}.\"\n",
    "    m = len(matrix[0])\n",
    "    if m == 0:\n",
    "        return False, f\"The matrix must have at least one column. The correct matrix is {gold_matrix}.\"\n",
    "    for i in range(n):\n",
    "        if not isinstance(matrix[i], list):\n",
    "            return False, f\"The {i}-th row must be a List[int]. The correct matrix is {gold_matrix}.\"\n",
    "        if len(matrix[i]) != m:\n",
    "            return (\n",
    "                False,\n",
    "                f\"The matrix is staggered. Row 0 has {m} columns, but row {i} has {len(matrix[i])} columns. The correct matrix is {gold_matrix}.\",\n",
    "            )\n",
    "        for j in range(m):\n",
    "            if not isinstance(matrix[i][j], int):\n",
    "                return (\n",
    "                    False,\n",
    "                    f\"The {i}-th row, {j}-th column must be an int, found {type(matrix[i][j])}. The correct matrix is {gold_matrix}.\",\n",
    "                )\n",
    "\n",
    "    # Check consistency with gold matrix\n",
    "    gold_n = len(gold_matrix)\n",
    "    gold_m = len(gold_matrix[0])\n",
    "    if (n, m) != (gold_n, gold_m):\n",
    "        return (\n",
    "            False,\n",
    "            f\"The matrix has dimensions {n}x{m}, but the gold matrix has dimensions {gold_n}x{gold_m}. The correct matrix is {gold_matrix}.\",\n",
    "        )\n",
    "\n",
    "    same = True\n",
    "    wrong_indices = []\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if matrix[i][j] != gold_matrix[i][j]:\n",
    "                same = False\n",
    "                wrong_indices.append((i, j))\n",
    "    if same:\n",
    "        return True, f\"Your response is correct. The correct matrix is {gold_matrix}.\"\n",
    "    else:\n",
    "        if len(wrong_indices) < 10:\n",
    "            return (\n",
    "                False,\n",
    "                f\"The matrix is incorrect. The following indices are incorrect: {wrong_indices}. The correct matrix is {gold_matrix}.\",\n",
    "            )\n",
    "        else:\n",
    "            return False, f\"The matrix is incorrect. The correct matrix is {gold_matrix}.\"\n",
    "\n",
    "\n",
    "def metric_fn(example, pred, trace=None):\n",
    "    task_inputs = example.test_inputs\n",
    "    gold_task_outputs = example.test_outputs\n",
    "    pred_task_outputs = pred.test_outputs\n",
    "\n",
    "    if not isinstance(pred_task_outputs, list):\n",
    "        return dspy.Prediction(\n",
    "            score=0,\n",
    "            feedback=f\"The response must be a List[List[List[int]]]. The correct response is {gold_task_outputs}.\",\n",
    "        )\n",
    "\n",
    "    valids = []\n",
    "    feedbacks = []\n",
    "    feedback = \"\"\n",
    "    if len(task_inputs) != len(pred_task_outputs):\n",
    "        feedback = f\"The number of output matrices ({len(pred_task_outputs)}) must match the number of input matrices ({len(task_inputs)}). The correct response is {gold_task_outputs}.\"\n",
    "        return dspy.Prediction(score=0, feedback=feedback)\n",
    "    for i, (input, gold_output, pred_output) in enumerate(\n",
    "        zip(task_inputs, gold_task_outputs, pred_task_outputs, strict=False)\n",
    "    ):\n",
    "        is_valid, feedback = is_valid_matrix(pred_output, gold_output)\n",
    "        valids.append(is_valid)\n",
    "        feedbacks.append(f\"Feedback on test input {i}: {feedback}\")\n",
    "\n",
    "    score = sum(valids) / len(valids)\n",
    "    feedback_text = \"\\n\".join(feedbacks)\n",
    "    return dspy.Prediction(score=score, feedback=feedback_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a44ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = input(\"GEMINI_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5729c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflection_lm = dspy.LM(model=\"openai/gpt-4.1\", max_tokens=32000, temperature=1)  # temperature=1\n",
    "\n",
    "reflection_lm = dspy.LM(model=\"gemini/gemini-2.5-pro\", max_tokens=32000, api_key=gemini_api_key)\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=dspy.LM(model=\"gemini/gemini-2.5-pro\", max_tokens=32000, api_key=gemini_api_key),\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=80,\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e136ebb",
   "metadata": {},
   "source": [
    "Let's evaluate the base program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4019d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(trainset)\n",
    "\n",
    "test_set = testset\n",
    "val_set = trainset[-200:]\n",
    "train_set = [ex for ex in trainset[:-200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947efd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "823d5d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:07 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:07 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/30 04:55:07 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/30 04:55:07 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/30 04:55:07 INFO dspy.evaluate.evaluate: Average Metric: 176.0 / 400 (44.0%)\n"
     ]
    }
   ],
   "source": [
    "o_base = adapter.evaluate(test_set, {\"program\": program_src})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51ee18",
   "metadata": {},
   "source": [
    "The base program obtains a score of 55.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c6097",
   "metadata": {},
   "source": [
    "Let's launch the GEPA optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60050bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GEPA Optimization:   0%|                                                                                                                              | 0/4000 [00:00<?, ?rollouts/s]2025/08/30 05:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 05:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 05:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 05:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 05:29:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 05:29:59 WARNING dspy.utils.parallelizer: SIGINT received. Cancelling.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgepa\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m o = \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed_candidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprogram\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogram_src\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreflection_lm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflection_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_metric_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplay_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Repos/gepa_dev/gepa/src/gepa/api.py:200\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(seed_candidate, trainset, valset, adapter, task_lm, reflection_lm, candidate_selection_strategy, skip_perfect_score, reflection_minibatch_size, perfect_score, use_merge, max_merge_invocations, max_metric_calls, logger, run_dir, use_wandb, wandb_api_key, wandb_init_kwargs, track_best_outputs, display_progress_bar, seed, raise_on_exception)\u001b[39m\n\u001b[32m    173\u001b[39m     merge_proposer = MergeProposer(\n\u001b[32m    174\u001b[39m         logger=logger,\n\u001b[32m    175\u001b[39m         valset=valset,\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m         rng=rng,\n\u001b[32m    180\u001b[39m     )\n\u001b[32m    182\u001b[39m engine = GEPAEngine(\n\u001b[32m    183\u001b[39m     run_dir=run_dir,\n\u001b[32m    184\u001b[39m     evaluator=evaluator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m     raise_on_exception=raise_on_exception,\n\u001b[32m    199\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m state = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m result = GEPAResult.from_state(state)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Repos/gepa_dev/gepa/src/gepa/core/engine.py:144\u001b[39m, in \u001b[36mGEPAEngine.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mvalset must be provided to GEPAEngine.run()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Initialize state (keeps your previous semantics)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m state = \u001b[43minitialize_gepa_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed_candidate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseed_candidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalset_evaluator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_val_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrack_best_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_best_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state.pareto_front_valset) == \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.valset)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_wandb:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Repos/gepa_dev/gepa/src/gepa/core/state.py:176\u001b[39m, in \u001b[36minitialize_gepa_state\u001b[39m\u001b[34m(run_dir, logger, seed_candidate, valset_evaluator, track_best_outputs)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m     num_evals_run = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     valset_out = \u001b[43mvalset_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    178\u001b[39m         write_eval_output_to_directory(valset_out, os.path.join(run_dir, \u001b[33m\"\u001b[39m\u001b[33mgenerated_best_outputs_valset\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Repos/gepa_dev/gepa/src/gepa/core/engine.py:80\u001b[39m, in \u001b[36mGEPAEngine._val_evaluator.<locals>.<lambda>\u001b[39m\u001b[34m(prog)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_val_evaluator\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Callable[[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[RolloutOutput], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]]:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.valset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m prog: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprog\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Repos/gepa_dev/gepa/src/gepa/api.py:168\u001b[39m, in \u001b[36moptimize.<locals>.evaluator\u001b[39m\u001b[34m(inputs, prog)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluator\u001b[39m(inputs, prog):\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     eval_out = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_traces\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_out.outputs, eval_out.scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Repos/gepa_dev/gepa/src/gepa/adapters/dspy_full_program_adapter/full_program_adapter.py:121\u001b[39m, in \u001b[36mDspyAdapter.evaluate\u001b[39m\u001b[34m(self, batch, candidate, capture_traces)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    112\u001b[39m     evaluator = Evaluate(\n\u001b[32m    113\u001b[39m         devset=batch,\n\u001b[32m    114\u001b[39m         metric=\u001b[38;5;28mself\u001b[39m.metric_fn,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m         max_errors=\u001b[38;5;28mlen\u001b[39m(batch) * \u001b[32m100\u001b[39m,\n\u001b[32m    120\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     res = \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     outputs = [r[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res.results]\n\u001b[32m    123\u001b[39m     scores = [r[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res.results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/dkVgzzPSvERsvAw20pkDb/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/dkVgzzPSvERsvAw20pkDb/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:162\u001b[39m, in \u001b[36mEvaluate.__call__\u001b[39m\u001b[34m(self, program, metric, devset, num_threads, display_progress, display_table, callback_metadata)\u001b[39m\n\u001b[32m    159\u001b[39m     score = metric(example, prediction)\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, score\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m results = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devset) == \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[32m    165\u001b[39m results = [((dspy.Prediction(), \u001b[38;5;28mself\u001b[39m.failure_score) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/dkVgzzPSvERsvAw20pkDb/lib/python3.11/site-packages/dspy/utils/parallelizer.py:48\u001b[39m, in \u001b[36mParallelExecutor.execute\u001b[39m\u001b[34m(self, function, data)\u001b[39m\n\u001b[32m     46\u001b[39m tqdm.tqdm._instances.clear()\n\u001b[32m     47\u001b[39m wrapped = \u001b[38;5;28mself\u001b[39m._wrap_function(function)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/dkVgzzPSvERsvAw20pkDb/lib/python3.11/site-packages/dspy/utils/parallelizer.py:149\u001b[39m, in \u001b[36mParallelExecutor._execute_parallel\u001b[39m\u001b[34m(self, function, data)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_done():\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m done, not_done = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m    151\u001b[39m     futures_set.remove(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:305\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(fs, timeout, return_when)\u001b[39m\n\u001b[32m    301\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[32m    303\u001b[39m     waiter = _create_and_install_waiters(fs, return_when)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m f._condition:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/dkVgzzPSvERsvAw20pkDb/lib/python3.11/site-packages/dspy/utils/parallelizer.py:109\u001b[39m, in \u001b[36mParallelExecutor._execute_parallel.<locals>.interrupt_manager.<locals>.handler\u001b[39m\u001b[34m(sig, frame)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m.cancel_jobs.set()\n\u001b[32m    108\u001b[39m logger.warning(\u001b[33m\"\u001b[39m\u001b[33mSIGINT received. Cancelling.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43morig_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from gepa import optimize\n",
    "\n",
    "o = optimize(\n",
    "    seed_candidate={\"program\": program_src},\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    "    adapter=adapter,\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0],\n",
    "    max_metric_calls=4000,\n",
    "    display_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c60b8",
   "metadata": {},
   "source": [
    "Let's see the DSPy program found by GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8a2dc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import dspy\n",
      "from typing import List, Tuple, Optional\n",
      "import pydantic\n",
      "import copy\n",
      "import traceback\n",
      "\n",
      "# Define type aliases and pydantic models for clarity and structure.\n",
      "MATRIX = List[List[int]]\n",
      "\n",
      "class TrainingExample(pydantic.BaseModel):\n",
      "    input: MATRIX\n",
      "    output: MATRIX\n",
      "\n",
      "# --- Signatures ---\n",
      "\n",
      "class HypothesizeRule(dspy.Signature):\n",
      "    \"\"\"\n",
      "    Analyze the provided input/output matrix pairs from the Abstraction and Reasoning Corpus (ARC).\n",
      "    Deduce the single, underlying transformation rule that converts each input matrix to its corresponding output matrix.\n",
      "    Describe this rule in clear, step-by-step, unambiguous English. Focus on the logic, not Python code.\n",
      "\n",
      "    **Successful Strategies to Consider:**\n",
      "    - **Start Simple:** First, check for simple rules. Is there a global transformation (e.g., rotation, reflection)? Is the output a subgrid of the input? Is a single color being replaced?\n",
      "    - **Look for Separators:** Check if the grid is partitioned by separator lines (e.g., rows or columns that are all one color, usually black). The transformation might be applied to each section independently.\n",
      "    - **Identify Objects:** Group contiguous non-background pixels into objects. Analyze how these objects are created, destroyed, or modified. Consider their properties: color, shape, size, position.\n",
      "    - **Find Marker Points:** Look for special 'marker' pixels (e.g., a uniquely colored pixel) that might define the geometry of an operation, like the corners of a shape to be drawn.\n",
      "    - **Relate Input to Output:** How do the properties of the input grid (dimensions, colors, object counts) relate to the output grid?\n",
      "    - **Avoid Over-complication:** Propose the simplest rule that explains ALL training examples. Do not suggest overly complex mathematical or recursive patterns unless absolutely necessary and supported by every example.\n",
      "    \"\"\"\n",
      "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"A list of input/output pairs demonstrating the transformation rule.\")\n",
      "    rule_description: str = dspy.OutputField(desc=\"A step-by-step English description of the transformation rule.\")\n",
      "\n",
      "class ImplementRule(dspy.Signature):\n",
      "    \"\"\"\n",
      "    You are an expert programmer. Your task is to write a single, self-contained Python function based on a provided rule description and example pairs.\n",
      "\n",
      "    **Function Requirements:**\n",
      "    - The function must be named `transform_matrix`.\n",
      "    - It must accept one argument: `matrix`, which is a list of lists of integers (representing the input grid).\n",
      "    - It must return a list of lists of integers (representing the transformed output grid).\n",
      "    - The function should not use any external libraries except for `copy` if needed (e.g., `import copy; new_matrix = copy.deepcopy(matrix)`).\n",
      "    - Your output must be ONLY the Python code for the function. Do not include any explanations, comments outside the function, or markdown formatting like ```python.\n",
      "\n",
      "    **Successful Strategies to Consider:**\n",
      "    - **Robust Parsing:** When rules involve sub-regions (e.g., quadrants, objects), write code that can robustly find their boundaries. Don't assume boundaries are perfectly aligned or can be found by checking just the first row/column.\n",
      "    - **Iterative Processes:** Some rules are applied repeatedly until the grid no longer changes. Consider using a `while` loop that continues as long as modifications are being made in a pass.\n",
      "    - **Consistent Output Shape:** When constructing the output grid row by row (e.g., from sections), ensure each generated row has the same length to avoid creating a staggered/jagged matrix.\n",
      "    - **Handling Edge Cases:** Ensure your code handles empty matrices or matrices with unexpected dimensions gracefully.\n",
      "\n",
      "    **Example of a Correctly Formatted Output:**\n",
      "    def transform_matrix(matrix: list[list[int]]) -> list[list[int]]:\n",
      "        # Your implementation here\n",
      "        # For example, find the most frequent color and fill the grid\n",
      "        from collections import Counter\n",
      "        import itertools\n",
      "        \n",
      "        if not matrix or not matrix[0]:\n",
      "            return []\n",
      "            \n",
      "        counts = Counter(itertools.chain.from_iterable(matrix))\n",
      "        if counts:\n",
      "            # Handle ties by picking the smaller number value\n",
      "            most_common_color = sorted(counts.items(), key=lambda item: (-item[1], item[0]))[0][0]\n",
      "        else:\n",
      "            return []\n",
      "\n",
      "        height = len(matrix)\n",
      "        width = len(matrix[0])\n",
      "        \n",
      "        return [[most_common_color for _ in range(width)] for _ in range(height)]\n",
      "    \"\"\"\n",
      "    rule_description: str = dspy.InputField(desc=\"The English description of the rule to implement.\")\n",
      "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"A list of input/output pairs to use as a reference for implementation.\")\n",
      "    python_function: str = dspy.OutputField(desc=\"A string containing a single Python function `transform_matrix` that implements the rule.\")\n",
      "\n",
      "class RefineCode(dspy.Signature):\n",
      "    \"\"\"\n",
      "    You are a senior programmer debugging a Python function. You will be given a rule description, training examples, the buggy Python code, and feedback on why it failed.\n",
      "    Your task is to fix the code so that it correctly implements the rule and passes the training examples.\n",
      "    The refined function must adhere to all the original requirements.\n",
      "\n",
      "    **Function Requirements:**\n",
      "    - The function must be named `transform_matrix`.\n",
      "    - It must accept one argument: `matrix`, which is a list of lists of integers.\n",
      "    - It must return a list of lists of integers.\n",
      "    - The function should not use any external libraries except for `copy`.\n",
      "    - Your output must be ONLY the Python code for the function. Do not include any explanations or markdown formatting.\n",
      "    \"\"\"\n",
      "    rule_description: str = dspy.InputField(desc=\"The English description of the rule to implement.\")\n",
      "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"A list of input/output pairs the function must correctly handle.\")\n",
      "    buggy_code: str = dspy.InputField(desc=\"The Python code that failed verification.\")\n",
      "    feedback: str = dspy.InputField(desc=\"A description of the error or the discrepancy between the function's output and the expected output for a training example.\")\n",
      "    refined_python_function: str = dspy.OutputField(desc=\"A string containing the corrected, single Python function `transform_matrix`.\")\n",
      "\n",
      "# --- Custom Module with Self-Correction ---\n",
      "\n",
      "class ARCSolver(dspy.Module):\n",
      "    \"\"\"A module that solves ARC tasks by hypothesizing a rule, generating code, and refining it through verification.\"\"\"\n",
      "    def __init__(self, max_retries=2):\n",
      "        super().__init__()\n",
      "        self.max_retries = max_retries\n",
      "        self.rule_hypothesizer = dspy.ChainOfThought(HypothesizeRule)\n",
      "        self.code_implementer = dspy.Predict(ImplementRule)\n",
      "        self.code_refiner = dspy.Predict(RefineCode)\n",
      "\n",
      "    def _execute_and_verify(self, python_code: str, examples: List[TrainingExample]) -> Tuple[bool, Optional[str]]:\n",
      "        \"\"\"\n",
      "        Executes the generated Python code and verifies its correctness against training examples.\n",
      "        Returns a tuple: (is_correct, feedback_message).\n",
      "        \"\"\"\n",
      "        local_scope = {}\n",
      "        try:\n",
      "            exec(python_code, globals(), local_scope)\n",
      "            transform_func = local_scope.get('transform_matrix')\n",
      "            if not callable(transform_func):\n",
      "                return False, \"The generated code does not define a callable function named 'transform_matrix'.\"\n",
      "        except Exception as e:\n",
      "            return False, f\"Code compilation failed with an error: {e}\\n{traceback.format_exc()}\"\n",
      "\n",
      "        for i, example_obj in enumerate(examples):\n",
      "            # Robustly handle both Pydantic models and dicts, as DSPy can convert them.\n",
      "            if isinstance(example_obj, dict):\n",
      "                try:\n",
      "                    example = TrainingExample(**example_obj)\n",
      "                except pydantic.ValidationError as e:\n",
      "                    return False, f\"Failed to parse training example {i} from dict: {e}\"\n",
      "            else:\n",
      "                example = example_obj\n",
      "\n",
      "            try:\n",
      "                input_copy = copy.deepcopy(example.input)\n",
      "                predicted_output = transform_func(input_copy)\n",
      "                if predicted_output != example.output:\n",
      "                    feedback = (\n",
      "                        f\"Verification failed on training example {i}.\\n\"\n",
      "                        f\"Input:\\n{example.input}\\n\"\n",
      "                        f\"Expected Output:\\n{example.output}\\n\"\n",
      "                        f\"Actual Output:\\n{predicted_output}\"\n",
      "                    )\n",
      "                    return False, feedback\n",
      "            except Exception as e:\n",
      "                feedback = (\n",
      "                    f\"An exception occurred during execution on training example {i}: {e}\\n\"\n",
      "                    f\"Input was:\\n{example.input}\\n\"\n",
      "                    f\"{traceback.format_exc()}\"\n",
      "                )\n",
      "                return False, feedback\n",
      "        \n",
      "        return True, None\n",
      "\n",
      "    def forward(self, training_examples: List[TrainingExample], test_inputs: List[MATRIX]):\n",
      "        # Step 1: Generate an English description of the transformation rule.\n",
      "        hypothesis = self.rule_hypothesizer(training_examples=training_examples)\n",
      "        \n",
      "        # Step 2: Generate the initial Python code.\n",
      "        prediction = self.code_implementer(\n",
      "            rule_description=hypothesis.rule_description,\n",
      "            training_examples=training_examples\n",
      "        )\n",
      "        python_code = prediction.python_function\n",
      "\n",
      "        # Step 3: Verify and refine the code in a loop.\n",
      "        for _ in range(self.max_retries):\n",
      "            is_correct, feedback = self._execute_and_verify(python_code, training_examples)\n",
      "            if is_correct:\n",
      "                break\n",
      "            \n",
      "            refinement = self.code_refiner(\n",
      "                rule_description=hypothesis.rule_description,\n",
      "                training_examples=training_examples,\n",
      "                buggy_code=python_code,\n",
      "                feedback=feedback\n",
      "            )\n",
      "            python_code = refinement.refined_python_function\n",
      "        \n",
      "        # Step 4: Execute the final code on test inputs with robust fallbacks.\n",
      "        fallback_outputs = [copy.deepcopy(matrix) for matrix in test_inputs]\n",
      "        local_scope = {}\n",
      "        try:\n",
      "            exec(python_code, globals(), local_scope)\n",
      "            transform_func = local_scope.get('transform_matrix')\n",
      "            if not callable(transform_func):\n",
      "                return dspy.Prediction(test_outputs=fallback_outputs)\n",
      "\n",
      "            solved_outputs = []\n",
      "            for test_matrix in test_inputs:\n",
      "                try:\n",
      "                    input_copy = copy.deepcopy(test_matrix)\n",
      "                    result = transform_func(input_copy)\n",
      "                    solved_outputs.append(result)\n",
      "                except Exception:\n",
      "                    solved_outputs.append(copy.deepcopy(test_matrix))\n",
      "            \n",
      "            return dspy.Prediction(test_outputs=solved_outputs)\n",
      "\n",
      "        except Exception:\n",
      "            return dspy.Prediction(test_outputs=fallback_outputs)\n",
      "\n",
      "# The overall task signature, defining the final inputs and outputs of the program.\n",
      "class SolveTaskSignature(dspy.Signature):\n",
      "    \"\"\"\n",
      "    Given a set of training examples demonstrating a matrix transformation,\n",
      "    apply the same transformation to a new set of test matrices.\n",
      "    \"\"\"\n",
      "    training_examples: List[TrainingExample] = dspy.InputField(description=\"Input and output examples demonstrating the task to be performed.\")\n",
      "    test_inputs: List[MATRIX] = dspy.InputField(description=\"Input matrices to be solved following the task described in the training examples.\")\n",
      "    test_outputs: List[MATRIX] = dspy.OutputField(description=\"Output matrices corresponding to the test inputs.\")\n",
      "\n",
      "# The final program is an instance of our new, more robust module.\n",
      "program = ARCSolver()\n"
     ]
    }
   ],
   "source": [
    "print(o.best_candidate[\"program\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5172bd",
   "metadata": {},
   "source": [
    "Evaluating the optimized program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b16e12a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/30 04:55:15 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "GEPA Optimization:   6%|██████▎                                                                                                     | 236/4000 [31:29:39<502:18:26, 480.42s/rollouts]\n",
      "2025/08/30 04:55:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:22 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:23 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/08/30 04:55:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:24 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=32000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/08/30 04:55:24 INFO dspy.evaluate.evaluate: Average Metric: 198.0 / 400 (49.5%)\n"
     ]
    }
   ],
   "source": [
    "o_opt = adapter.evaluate(test_set, o.best_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83f611",
   "metadata": {},
   "source": [
    "We see it going from **67% to 93%** in just a few rounds of optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a115f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"arc_agi_gepa_result_gemini.bin\", \"wb\") as f:\n",
    "    pickle.dump(o, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
